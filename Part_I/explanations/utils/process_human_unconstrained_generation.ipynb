{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6cee62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6324a119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Notebook to process received human data\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json \n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pylab as plt \n",
    "import seaborn as sns\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.gridspec as gridspec\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1079d10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"unconstrained\"\n",
    "\n",
    "data_pth = f\"../data/generations/generate-explanations-{task}.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94b5788",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_pth)\n",
    "\n",
    "# extract subj ids\n",
    "all_subjs = set(df.PROLIFIC_PID)\n",
    "\n",
    "print(\"Num subjs: \", len(all_subjs), \", Num rows: \", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fa8c994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition Num:  1  Count:  11\n",
      "Condition Num:  2  Count:  10\n",
      "Condition Num:  3  Count:  10\n",
      "Condition Num:  4  Count:  10\n"
     ]
    }
   ],
   "source": [
    "# check num subjs per condition \n",
    "subjs_per_condition = {}\n",
    "for subj_id in all_subjs: \n",
    "    subj_df = df.loc[df.PROLIFIC_PID == subj_id].reset_index()\n",
    "    condition_num = int(subj_df.condition[0])\n",
    "    if condition_num not in subjs_per_condition: subjs_per_condition[condition_num] = [subj_id]\n",
    "    else: subjs_per_condition[condition_num].append(subj_id)\n",
    "\n",
    "for cond_num in sorted(subjs_per_condition.keys()): \n",
    "    print(\"Condition Num: \", cond_num, \" Count: \", len(subjs_per_condition[cond_num]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63164cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out trial type to only include goal text + ratings\n",
    "# key columns: \n",
    "# - \"PROLIFIC_PID\": subj_id\n",
    "# - \"prompt\": goal\n",
    "# - \"responses\": plan or rating (depending on task)\n",
    "# - \"rt\": reaction time (in milliseconds)\n",
    "goal_ratings_df = df[(df.trial_type == 'survey-likert')].reset_index()\n",
    "generated_explanations_df = df[(df.trial_type == 'survey-text') \n",
    "                     & (df.task==f\"generate, {task}\")].reset_index() # remove comments (b/c same data type)\n",
    "generated_explanations_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b90523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the full prompts for each batch using a sample subj per\n",
    "# helpful for downstream plotting/decomp. in analysis\n",
    "batched_stim = {cond_num: [] for cond_num in subjs_per_condition.keys()}\n",
    "for cond_num in subjs_per_condition.keys():\n",
    "    sample_subj = subjs_per_condition[cond_num][0]\n",
    "    prompts = list(generated_explanations_df.loc[generated_explanations_df.PROLIFIC_PID == sample_subj].prompt)\n",
    "    batched_stim[cond_num] = prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06b9c050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'run_id', 'condition', 'view_history', 'rt', 'trial_type',\n",
       "       'trial_index', 'time_elapsed', 'internal_node_id', 'PROLIFIC_PID',\n",
       "       'STUDY_ID', 'SESSION_ID', 'subject_id', 'study_id', 'session_id',\n",
       "       'response', 'question_order', 'phase', 'prompt', 'task', 'cause',\n",
       "       'result', 'recorded_at', 'ip', 'user_agent', 'device', 'browser',\n",
       "       'browser_version', 'platform', 'platform_version',\n",
       "       'source_code_version'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goal_ratings_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd7808ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save out explanations generated by each subject\n",
    "\n",
    "fragment = \"This could have happened because\"\n",
    "\n",
    "save_dir = f\"../exp_results/\"\n",
    "\n",
    "if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "\n",
    "def parse_explanation(raw_explanation): \n",
    "    # remove extraneous symbols from explanation \n",
    "    # (hacky b/c of way original data was saved)\n",
    "    explanation = raw_explanation.split(\"\\\"Explanation\\\":\")[-1].split(\"}\")[0]\n",
    "    return explanation.split(\":\\\"\")[-1].split(\"\\\"}\")[0].replace(\"\\\\n\", \"<br />\") \n",
    "\n",
    "def save_subj_data(subj_id, data_df, f): \n",
    "    # extract data only for subject\n",
    "    subj_df = data_df.loc[data_df.PROLIFIC_PID == subj_id]#.reset_index()\n",
    "    f.write(f'\\nPID: {subj_id}\\n')\n",
    "    for goal, raw_explanation in zip(subj_df.prompt, subj_df.response): \n",
    "        explanation = parse_explanation(raw_explanation)[1:] # remove starting \"\n",
    "        pre_frag = f\"{explanation[0].lower()}{explanation[1:]}\"\n",
    "        if pre_frag[:32] == \"this could have happened because\": \n",
    "            pre_frag = pre_frag[32:]\n",
    "        explanation = f\"\\\"{fragment} {pre_frag}\" # include the starter text\n",
    "        f.write(\n",
    "            f'\\n\\tScenario: {goal}\\n\\tExplanation: {explanation}\\n'\n",
    "        )\n",
    "    \n",
    "\n",
    "subj_ids = sorted(list(all_subjs)) # ensure same order for consistency\n",
    "filepth = f\"{save_dir}/exp_per_subj_{task}.txt\"\n",
    "f = open(filepth, 'w')\n",
    "f.write(\"Generated Explanations per Subject\\n\")\n",
    "\n",
    "for subj_id in subj_ids: \n",
    "    save_subj_data(subj_id, generated_explanations_df, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45f29520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of explanations per goal \n",
    "random.seed(10)\n",
    "parsed_explanations_per_goal = {}\n",
    "all_goals = sorted(list(set(generated_explanations_df.prompt)))\n",
    "n_keep = 10\n",
    "pre_frags=[]\n",
    "for full_goal in all_goals:\n",
    "    goal_df = generated_explanations_df.loc[generated_explanations_df.prompt == full_goal]\n",
    "    if len(goal_df.response) < 10: \n",
    "        print(full_goal, len(goal_df.response))\n",
    "    parsed_explanations = []\n",
    "    for raw_explanation in goal_df.response: \n",
    "        explanation = parse_explanation(raw_explanation)[1:]\n",
    "        pre_frag = f\"{explanation[0].lower()}{explanation[1:]}\"\n",
    "        if pre_frag[:32] == \"this could have happened because\": \n",
    "            pre_frag = pre_frag[32:]\n",
    "        pre_frags.append(pre_frag)\n",
    "        explanation = f\"\\\"{fragment} {pre_frag}\" # include the starter text\n",
    "        \n",
    "#         explanation = f\"\\\"{fragment} {explanation[0].lower()}{explanation[1:]}\" # include the starter text\n",
    "        parsed_explanations.append(explanation)\n",
    "    # subsample down to 10\n",
    "    parsed_explanations_per_goal[full_goal] = random.sample(parsed_explanations, n_keep) # TODO: add back samples later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "488bd0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save out plans per goal (aggregate over subjects)\n",
    "pre_frags = []\n",
    "\n",
    "def save_goal_data(goal, data_df, f): \n",
    "    # extract data only corresponding to specific goal\n",
    "    goal_df = data_df.loc[data_df.prompt == goal]#.reset_index()\n",
    "    constraint = goal_df.constraint.iloc[0]\n",
    "    f.write(f'\\nScenario: {goal}\\n')\n",
    "    if len(goal_df.response) < 10: \n",
    "        print(goal, len(goal_df.response))\n",
    "    for raw_explanation in goal_df.response: \n",
    "        explanation = parse_explanation(raw_explanation)[1:] # remove starting \"\n",
    "        pre_frag = f\"{explanation[0].lower()}{explanation[1:]}\"\n",
    "        if pre_frag[:32] == \"this could have happened because this could have happened because\": \n",
    "            print(pre_frag)\n",
    "            pre_frag = pre_frag[32:]\n",
    "        pre_frags.append(pre_frag)\n",
    "        explanation = f\"\\\"{fragment} {pre_frag}\" # include the starter text\n",
    "#         explanation = f\"\\\"{fragment} {explanation[0].lower()}{explanation[1:]}\" # include the starter text\n",
    "        f.write(\n",
    "            f'\\n\\tExplanation: {explanation}\\n'\n",
    "        )\n",
    "        \n",
    "filepth = f\"{save_dir}/exp_per_scenario_{task}.txt\"\n",
    "f = open(filepth, 'w')\n",
    "f.write(\"Generated Explanations per Scenario\\n\")\n",
    "full_goals = sorted(parsed_explanations_per_goal.keys())\n",
    "for goal in full_goals: \n",
    "    f.write(f'\\nScenario: {goal}\\n')\n",
    "    for plan in parsed_explanations_per_goal[goal]: \n",
    "        f.write(\n",
    "            f'\\n\\tExplanation: {plan}\\n'\n",
    "        )\n",
    "    \n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45029df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean num tokens: 25.96785714285714\n",
      "  Min num tokens: 10\n",
      "  Max num tokens: 163\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# compute number of tokens per plan\n",
    "n_tokens = []\n",
    "tot_plans = 0\n",
    "for goal, plans in parsed_explanations_per_goal.items(): \n",
    "    plan = plan[34:] # remove \"This could have happened because\" # since added\n",
    "    n_tokens.extend([len(tokenizer(plan)['input_ids']) for plan in plans])\n",
    "    \n",
    "print(f\"Mean num tokens: {np.mean(n_tokens)}\") \n",
    "print(f\"  Min num tokens: {np.min(n_tokens)}\") \n",
    "print(f\"  Max num tokens: {np.max(n_tokens)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65cad143",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# extract ratings per goal \n",
    "freqX_ratings_per_goal = {}\n",
    "freqConditional_ratings_per_goal = {}\n",
    "for goal, rating_str in zip(goal_ratings_df[\"prompt\"],goal_ratings_df[\"response\"]): \n",
    "    rating = json.loads(rating_str)\n",
    "    freqX_rating = rating[\"freqX\"]\n",
    "    freqConditional_rating = rating[\"freqY_givenX\"]\n",
    "    if goal not in freqX_ratings_per_goal: \n",
    "        freqX_ratings_per_goal[goal] = [freqX_rating]\n",
    "        freqConditional_ratings_per_goal[goal] = [freqConditional_rating]\n",
    "    else: \n",
    "        freqX_ratings_per_goal[goal].append(freqX_rating)\n",
    "        freqConditional_ratings_per_goal[goal].append(freqConditional_rating)\n",
    "\n",
    "all_stims = sorted(freqX_ratings_per_goal.keys())\n",
    "mean_freqX = [np.mean(freqX_ratings_per_goal[stim]) for stim in all_stims]\n",
    "mean_freqY_givenX = [np.mean(freqConditional_ratings_per_goal[stim]) for stim in all_stims]\n",
    "stim_rating_df = pd.DataFrame({\"Scenario\": all_stims, \"Freq X\": mean_freqX, \"Freq X Given Y\": mean_freqY_givenX})\n",
    "stim_rating_df.to_csv(\"explanation_typicality.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
