{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6324a119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Process human plan data for Conditions 2 and 3 (constrained)\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json \n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pylab as plt \n",
    "import seaborn as sns\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.gridspec as gridspec\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1079d10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"constrained\"\n",
    "\n",
    "data_pth = f\"../data/generations/generate-explanations-{task}.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94b5788",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_pth)\n",
    "\n",
    "# extract subj ids\n",
    "all_subjs = set(df.PROLIFIC_PID)\n",
    "\n",
    "print(\"Num subjs: \", len(all_subjs), \", Num rows: \", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724f3ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fa8c994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition Num:  1  Count:  22\n",
      "Condition Num:  2  Count:  16\n",
      "Condition Num:  3  Count:  14\n",
      "Condition Num:  4  Count:  13\n",
      "Condition Num:  5  Count:  15\n",
      "Condition Num:  6  Count:  15\n",
      "Condition Num:  7  Count:  15\n",
      "Condition Num:  8  Count:  15\n"
     ]
    }
   ],
   "source": [
    "# check num subjs per condition \n",
    "subjs_per_condition = {}\n",
    "for subj_id in all_subjs: \n",
    "    subj_df = df.loc[df.PROLIFIC_PID == subj_id].reset_index()\n",
    "    condition_num = int(subj_df.condition[0])\n",
    "    if condition_num not in subjs_per_condition: subjs_per_condition[condition_num] = [subj_id]\n",
    "    else: subjs_per_condition[condition_num].append(subj_id)\n",
    "\n",
    "for cond_num in sorted(subjs_per_condition.keys()): \n",
    "    print(\"Condition Num: \", cond_num, \" Count: \", len(subjs_per_condition[cond_num]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c51ac6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num scenarios:  28\n",
      "single-constraint -- Mean num tokens: 29.675\n",
      "  Min num tokens: 11\n",
      "  Max num tokens: 101\n",
      "Num scenarios:  28\n",
      "all-constraints -- Mean num tokens: 28.728571428571428\n",
      "  Min num tokens: 10\n",
      "  Max num tokens: 93\n"
     ]
    }
   ],
   "source": [
    "tasks = [\"single-constraint\",  \"all-constraints\"]\n",
    "fragment = \"This could have happened because\"\n",
    "constraint_fragment = \"However, the reason this happened was not that\"\n",
    "random.seed(10)\n",
    "\n",
    "save_dir = f\"../exp_results/\"\n",
    "\n",
    "if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    " \n",
    "\n",
    "def parse_explanation(raw_explanation): \n",
    "    # remove extraneous symbols from explanation \n",
    "    # (hacky b/c of way original data was saved)\n",
    "    explanation = raw_explanation.split(\"\\\"Explanation\\\":\")[-1].split(\"}\")[0]\n",
    "    return explanation.split(\":\\\"\")[-1].split(\"\\\"}\")[0].replace(\"\\\\n\", \"<br />\") \n",
    "\n",
    "def save_subj_data(subj_id, data_df, f, constraint_per_scenario): \n",
    "    # extract data only for subject\n",
    "    subj_df = data_df.loc[data_df.PROLIFIC_PID == subj_id]#.reset_index()\n",
    "    f.write(f'\\nPID: {subj_id}\\n')\n",
    "    for goal, raw_explanation in zip(subj_df.prompt, subj_df.response): \n",
    "        explanation = parse_explanation(raw_explanation)[1:] # remove starting \"\n",
    "        pre_frag = f\"{explanation[0].lower()}{explanation[1:]}\"\n",
    "        if pre_frag[:32] == \"this could have happened because\": \n",
    "            pre_frag = pre_frag[32:]\n",
    "        explanation = f\"\\\"{fragment} {pre_frag}\" # include the starter text\n",
    "        constraint = constraint_per_scenario[goal]\n",
    "        f.write(\n",
    "            f'\\n\\tScenario: {goal} {constraint_fragment} {constraint}.\\n\\tExplanation: {explanation}\\n'\n",
    "        )\n",
    "\n",
    "def save_goal_data(goal, data_df, f): \n",
    "    # extract data only corresponding to specific goal\n",
    "    goal_df = data_df.loc[data_df.prompt == goal]#.reset_index()\n",
    "    constraint = goal_df.constraint.iloc[0]\n",
    "    f.write(f'\\nScenario: {goal}\\n')\n",
    "    if len(goal_df.response) < 10: \n",
    "        print(goal, len(goal_df.response))\n",
    "    for raw_explanation in goal_df.response: \n",
    "        explanation = parse_explanation(raw_explanation)[1:] # remove starting \"\n",
    "        pre_frag = f\"{explanation[0].lower()}{explanation[1:]}\"\n",
    "        if pre_frag[:32] == \"this could have happened because this could have happened because\": \n",
    "            print(pre_frag)\n",
    "            pre_frag = pre_frag[32:]\n",
    "        pre_frags.append(pre_frag)\n",
    "        explanation = f\"\\\"{fragment} {pre_frag}\" # include the starter text\n",
    "#         explanation = f\"\\\"{fragment} {explanation[0].lower()}p{explanation[1:]}\" # include the starter text\n",
    "        f.write(\n",
    "            f'\\n\\tExplanation: {explanation}\\n'\n",
    "        )\n",
    "\n",
    "for task in tasks: \n",
    "    # filter out trial type to only include goal text + ratings\n",
    "    # key columns: \n",
    "    # - \"PROLIFIC_PID\": subj_id\n",
    "    # - \"prompt\": goal\n",
    "    # - \"responses\": plan or rating (depending on task)\n",
    "    # - \"rt\": reaction time (in milliseconds)\n",
    "    goal_ratings_df = df[(df.trial_type == 'survey-likert')].reset_index()\n",
    "    generated_explanations_df = df[(df.trial_type == 'survey-text') \n",
    "                         & (df.task==f\"generate, {task}\")].reset_index() # remove comments (b/c same data type)\n",
    "#     print(generated_explanations_df.head(3))\n",
    "    \n",
    "    # get a mapping of scenario to its associated constraint\n",
    "    constraint_per_scenario = {}\n",
    "    for scenario, constraint in zip(generated_explanations_df.prompt, generated_explanations_df.constraint): \n",
    "        if scenario not in constraint_per_scenario: \n",
    "            constraint_per_scenario[scenario] = constraint\n",
    "    print(\"Num scenarios: \", len(constraint_per_scenario.keys()))\n",
    "    \n",
    "    # store the full prompts for each batch using a sample subj per\n",
    "    # helpful for downstream plotting/decomp. in analysis\n",
    "    batched_stim = {cond_num: [] for cond_num in subjs_per_condition.keys()}\n",
    "    for cond_num in subjs_per_condition.keys():\n",
    "        sample_subj = subjs_per_condition[cond_num][0]\n",
    "        prompts = list(generated_explanations_df.loc[generated_explanations_df.PROLIFIC_PID == sample_subj].prompt)\n",
    "        batched_stim[cond_num] = prompts\n",
    "        \n",
    "\n",
    "    subj_ids = sorted(list(all_subjs)) # ensure same order for consistency\n",
    "    filepth = f\"{save_dir}/exp_per_subj_{task}.txt\"\n",
    "    f = open(filepth, 'w')\n",
    "    f.write(\"Generated Explanations per Subject\\n\")\n",
    "\n",
    "    for subj_id in subj_ids: \n",
    "        save_subj_data(subj_id, generated_explanations_df, f, constraint_per_scenario)\n",
    "    f.close()\n",
    "    \n",
    "    # get a list of explanations per goal \n",
    "    parsed_explanations_per_goal = {}\n",
    "    all_goals = sorted(list(set(generated_explanations_df.prompt)))\n",
    "    n_keep = 10\n",
    "    pre_frags=[]\n",
    "    for full_goal in all_goals:\n",
    "        goal_df = generated_explanations_df.loc[generated_explanations_df.prompt == full_goal]\n",
    "        if len(goal_df.response) < 10: \n",
    "            print(full_goal, len(goal_df.response), \" cond: \", goal_df.condition.iloc[0])\n",
    "        parsed_explanations = []\n",
    "        for raw_explanation in goal_df.response: \n",
    "            explanation = parse_explanation(raw_explanation)[1:]\n",
    "            pre_frag = f\"{explanation[0].lower()}{explanation[1:]}\"\n",
    "            if pre_frag[:32] == \"this could have happened because\": \n",
    "                pre_frag = pre_frag[32:]\n",
    "            pre_frags.append(pre_frag)\n",
    "            explanation = f\"\\\"{fragment} {pre_frag}\" # include the starter text\n",
    "            explanation = explanation.replace(\"  \", \" \") # remove double-spaces that may have been introduced w/ parse\n",
    "\n",
    "    #         explanation = f\"\\\"{fragment} {explanation[0].lower()}{explanation[1:]}\" # include the starter text\n",
    "            parsed_explanations.append(explanation)\n",
    "        # subsample down to 10\n",
    "        parsed_explanations_per_goal[full_goal] = random.sample(parsed_explanations, n_keep) \n",
    "        \n",
    "    # save out plans per goal (aggregate over subjects)\n",
    "    pre_frags = []\n",
    "\n",
    "\n",
    "\n",
    "    filepth = f\"{save_dir}/exp_per_scenario_{task}.txt\"\n",
    "    f = open(filepth, 'w')\n",
    "    f.write(\"Generated Explanations per Scenario\\n\")\n",
    "    full_goals = sorted(parsed_explanations_per_goal.keys())\n",
    "    for goal in full_goals: \n",
    "        constraint = constraint_per_scenario[goal]\n",
    "        f.write(f'\\nScenario: {goal} {constraint_fragment} {constraint}.\\n')\n",
    "        for plan in parsed_explanations_per_goal[goal]: \n",
    "            f.write(\n",
    "                f'\\n\\tExplanation: {plan}\\n'\n",
    "            )\n",
    "\n",
    "    f.close()    \n",
    "        \n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    # compute number of tokens per plan\n",
    "    n_tokens = []\n",
    "    tot_plans = 0\n",
    "    for goal, plans in parsed_explanations_per_goal.items(): \n",
    "        plan = plan[34:] # remove \"This could have happened because\" # since added\n",
    "        n_tokens.extend([len(tokenizer(plan)['input_ids']) for plan in plans])\n",
    "\n",
    "    print(f\"{task} -- Mean num tokens: {np.mean(n_tokens)}\") \n",
    "    print(f\"  Min num tokens: {np.min(n_tokens)}\") \n",
    "    print(f\"  Max num tokens: {np.max(n_tokens)}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
