{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a17b8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze humanness experiments (see how many rated < 2)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json \n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pylab as plt \n",
    "import seaborn as sns\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "def parse_rating(rating_str): \n",
    "    # cognition.run saves all data as strings\n",
    "    # need to parse dictionary from a string\n",
    "    # returns an int\n",
    "    rating = rating_str.split(\":\")[-1].split(\"}\")[0]\n",
    "    return int(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2352a913",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2\n",
    "\n",
    "data_pth = \"../data/humanness_ratings/rate-humanness-gpt-3-official-pilot.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8173f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_pth)\n",
    "\n",
    "# extract subj ids\n",
    "all_subjs = set(df.PROLIFIC_PID)\n",
    "\n",
    "\n",
    "print(\"Num subjs: \", len(all_subjs), \", Num rows: \", len(df))\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f373bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check num subjs per condition \n",
    "subjs_per_condition = {}\n",
    "for subj_id in all_subjs: \n",
    "    subj_df = df.loc[df.PROLIFIC_PID == subj_id].reset_index()\n",
    "    condition_num = int(subj_df.condition[0])\n",
    "    if condition_num not in subjs_per_condition: subjs_per_condition[condition_num] = [subj_id]\n",
    "    else: subjs_per_condition[condition_num].append(subj_id)\n",
    "\n",
    "for cond_num in sorted(subjs_per_condition.keys()): \n",
    "    print(\"Condition Num: \", cond_num, \" Count: \", len(subjs_per_condition[cond_num]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adab570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out to only include the ratings\n",
    "# e.g., any response that was of type \"rate_goodness\" \n",
    "rating_df = df[(df.task == \"rate_humanness\")].reset_index()\n",
    "# gather all goals\n",
    "all_goals = set(rating_df.prompt)\n",
    "print(\"Num goals: \", len(all_goals), \" rating df: \", len(rating_df))\n",
    "# keep only the first rating for people who saw a duplicate plan (due to accidental repeated planner)\n",
    "for subj_id in all_subjs:\n",
    "    subj_df = rating_df[rating_df.PROLIFIC_PID == subj_id]\n",
    "    rated_plans = set()\n",
    "    for idx, plan in zip(subj_df.index, subj_df.plan): \n",
    "        if plan in rated_plans: rating_df = rating_df.drop(idx)\n",
    "        else: rated_plans.add(plan)\n",
    "rating_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbba85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# find cases where raters differ by a large amount \n",
    "ratings_per_goal = {goal: {} for goal in all_goals}\n",
    "for goal, plan, rating_str, batch_idx in zip(rating_df.prompt, rating_df.plan, rating_df.response, rating_df.batch_idx): \n",
    "    rating = parse_rating(rating_str)\n",
    "    if plan not in ratings_per_goal[goal]: \n",
    "        ratings_per_goal[goal][plan] = [rating]\n",
    "    else: ratings_per_goal[goal][plan].append(rating)\n",
    "\n",
    "\n",
    "mean_rating_per_plan = {goal: [(plan, np.mean(ratings)) for plan, ratings in ratings_per_goal[goal].items()] for goal in ratings_per_goal.keys()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b79e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_prompt_df = rating_df[(rating_df.batch_idx == \"0\") |\n",
    "        (rating_df.batch_idx == \"1\")].reset_index(drop=True)\n",
    "\n",
    "many_prompt_df[\"rating\"] = [parse_rating(rating_str) for rating_str in many_prompt_df.response]\n",
    "\n",
    "rated_zero = many_prompt_df[many_prompt_df.rating == 0]\n",
    "len(many_prompt_df), len(rated_zero)\n",
    "\n",
    "rated_zero.to_csv(\"rated_zero_pilot.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3a552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_plans = []\n",
    "remove_plans = []\n",
    "\n",
    "mean_threshold = 2 \n",
    "\n",
    "for plan in set(many_prompt_df.plan): \n",
    "    plan_df = many_prompt_df[many_prompt_df.plan == plan].reset_index(drop=True)\n",
    "    mean_rating = np.mean(plan_df.rating)\n",
    "    if mean_rating < mean_threshold: \n",
    "        remove_plans.extend(set(plan_df.plan))\n",
    "    else: \n",
    "        # keep \n",
    "        keep_plans.append(plan_df)\n",
    "        \n",
    "keep_df = pd.concat(keep_plans)\n",
    "\n",
    "for goal in set(keep_df.prompt): \n",
    "    print(goal, \"Num unique plans: \", len(set(keep_df[keep_df.prompt == goal].plan)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b02cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â inspect the plans we're removing \n",
    "len(remove_plans), remove_plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bdc73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save keep_df and use for later\n",
    "# randomly sample 20 and divide into batches of 10 \n",
    "# call subsets 0, 1, 2, ... based on condition number\n",
    "\n",
    "save_dir = f\"../exp_results/gpt3_humanness\" \n",
    "     \n",
    "if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "\n",
    "num_plans_per_goal = 20 \n",
    "num_plans_per_batch = 10 \n",
    "\n",
    "plans = []\n",
    "goals = []\n",
    "goal_types = []\n",
    "ids = []\n",
    "batch_idxs = []\n",
    "\n",
    "all_goals = set(keep_df.prompt)\n",
    "for goal in all_goals: \n",
    "    goal_df = keep_df[keep_df.prompt == goal]\n",
    "    \n",
    "    goal_type = goal_df.goal_type.iloc[0]\n",
    "    \n",
    "    # subsample from the set of plans\n",
    "    poss_plans = set(goal_df.plan)\n",
    "    sampled_plans = np.random.choice(list(poss_plans), num_plans_per_goal, replace=False)\n",
    "    \n",
    "    plans_in_batch = np.random.choice(sampled_plans, num_plans_per_batch, replace = False)\n",
    "    plans.extend(plans_in_batch)\n",
    "    batch_idxs.extend([0 for _ in range(num_plans_per_batch)])\n",
    "    \n",
    "    # add other plans not in batch 0 to the next batch \n",
    "    # NOTE: this code is specific for two batches for now \n",
    "    other_plans = set(sampled_plans) - set(plans_in_batch)\n",
    "    plans.extend(other_plans)\n",
    "    batch_idxs.extend([1 for _ in range(num_plans_per_batch)])\n",
    "    \n",
    "    # add other meta data\n",
    "    goals.extend([goal for _ in range(num_plans_per_goal)])\n",
    "    goal_type = list(set(goal_df.goal_type) - {'\\\"'})[0]\n",
    "    goal_types.extend([goal_type for _ in range(num_plans_per_goal)])\n",
    "    ids.extend([\"gpt-3\" for _ in range(num_plans_per_goal)])\n",
    "    \n",
    "filtered_df = pd.DataFrame({\"goal\": goals, \"plan\": plans, \"id\": ids, \"batch_idx\": batch_idxs, \"goal_type\": goal_types})\n",
    "filtered_df.to_csv(f\"{save_dir}/max_prompt_filtered_plans_pilot.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74080c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sampled_plans), len(other_plans), len(plans_in_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470c4ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
